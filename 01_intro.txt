include::license.txt[]

:language: C

// A tiny introduction to asynchronous IO
非同期入出力に関してのちょっとした導入
----------------------------------------

// Most beginning programmers start with blocking IO calls.
// An IO call is _synchronous_ if, when you call it, it does not return
// until the operation is completed, or until enough time
// has passed that your network stack gives up.  When you call "connect()" on a TCP
// connection, for example, your operating system queues a SYN packet to
// the host on the other side of the TCP connection.  It does not return
// control back to your application until either it has received a SYN ACK
// packet from the opposite host, or until enough time has passed that it
// decides to give up.
大半の初心者プログラマはブロッキング、つまり、次の動作を妨げるような、入出力呼び出しから始めます。入出力呼び出しが _同期的である_ とは、もし、それを呼び出すと、動作が完了するか、ネットワーク・スタックが動作を中断させるほど十分な時間が経過するまで返ってくることがないことを指します。例えば、TCP接続の "connect()" を呼び出すと、オペレーティングシステムはTCP接続先ホストの接続キューにSYNパケットを登録します。この呼び出しでは、接続先ホストからSYN ACKパケットを受け取るか、処理を中断させるほど十分な時間が経過するまでアプリケーションに制御権を返しません。

// Here's an example of a really simple client using blocking network
// calls.  It opens a connection to www.google.com, sends it a simple
// HTTP request, and prints the response to stdout.
例としてブロッキング・ネットワーク呼び出しを用いた非常に簡単なクライアントを示します。クライアントは www.google.com への接続を開き、単純なHTTPリクエストを送ります。そして、レスポンスを標準出力に表示します。

//BUILD: SKIP
// .Example: A simple blocking HTTP client
.例: 単純なブロッキングHTTPクライアント
[code,C]
-------
include::examples_01/01_sync_webclient.c[]
-------

// All of the network calls in the code above are _blocking_: the
// gethostbyname does not return until it has succeeded or failed in
// resolving www.google.com; the connect does not return until it has
// connected; the recv calls do not return until they have received data
// or a close; and the send call does not return until it has at least
// flushed its output to the kernel's write buffers.
上で示したコードでは、すべてのネットワーク呼び出しは _ブロッキング_ です。つまり、gethostbynameは、www.google.comの名前解決に成功もしくは失敗するまでは戻ることはありません。connectは、接続が確立するまでは戻ることはありません。recv呼び出しは、データもしくは接続が切断されたことを受信するまでは戻ることはありません。そして、send呼び出しは、少なくても、出力データをカーネルの書き込みバッファにフラッシュするまでは戻ることはありません。

// Now, blocking IO is not necessarily evil.  If there's nothing else you
// wanted your program to do in the meantime, blocking IO will work fine
// for you.  But suppose that you need to write a program to handle
// multiple connections at once.  To make our example concrete: suppose
// that you want to read input from two connections, and you don't know
// which connection will get input first.  You can't say
// CHECK: wantedはwantの誤り?
ところで、ブロッキング入出力は必ずしも悪いというわけではありません。もし、プログラムの入出力中に他にしてほしいことがない場合は、ブロッキング入出力はうまく動作するでしょう。しかし、もし、一度に複数の接続を扱うプログラムを書く必要があるとしたらどうでしょう。具体的な例として、2つの接続からの入力を読み込みたいとしましょう。ただし、どちらの接続が最初の入力になるかはわからないとします。以下のコードはダメです。

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
// .Bad Example
.ダメな例
[code,C]
-------
/* This won't work. */
char buf[1024];
int i, n;
while (i_still_want_to_read()) {
    for (i=0; i<n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n==0)
            handle_close(fd[i]);
        else if (n<0)
            handle_error(fd[i], errno);
        else
            handle_input(fd[i], buf, n);
    }
}
-------
// because if data arrives on fd[2] first, your program won't even try
// reading from fd[2] until the reads from fd[0] and fd[1] have gotten some
// data and finished.
なぜなら、もし最初にfd[2]にデータが到着する場合、fd[0]とfd[1]からの読み込みで何かしらのデータを受け取るまで、プログラムはfd[2]から読み込むことすらしないからです。

// Sometimes people solve this problem with multithreading, or with
// multi-process servers.  One of the simplest ways to do multithreading
// is with a separate process (or thread) to deal with each connection.
// Since each connection has its own process, a blocking IO call that
// waits for one connection won't make any of the other connections'
// processes block.
たまに、この問題をマルチスレッドやマルチプロセスサーバを使って解決する人がいます。マルチスレッドにする最も簡単な方法の1つとして、それぞれの接続を処理するための別々のプロセス(もしくはスレッド)を持たせる方法があります。それぞれの接続は独自のプロセスを持つため、ある接続を待っているブロッキング入出力呼び出しが他の接続についてのプロセスをブロックしてしまうことはありません。

// Here's another example program.  It is a trivial server that listens
// for TCP connections on port 40713, reads data from its input one line
// at a time, and writes out the ROT13 obfuscation of line each as it
// arrives.  It uses the Unix fork() call to create a new process for
// each incoming connection.
もう1つのプログラム例を示します。40713ポートのTCP接続を待ち受けるありきたりなサーバで、入力から行単位でデータを読み込みます。そして、改行に到達する度に、行をROT13難読化したものを書き出します。プログラムはUnixのfork()呼び出しを使って、接続要求ごとに新しいプロセスを生成します。

//BUILD: SKIP
// .Example: Forking ROT13 server
.例: forkによるrot13サーバ
[code,C]
-------
include::examples_01/01_rot13_server_forking.c[]
-------

// So, do we have the perfect solution for handling multiple connections
// at once?  Can I stop writing this book and go work on something else
// now?  Not quite.  First off, process creation (and even thread
// creation) can be pretty expensive on some platforms.  In real life,
// you'd want to use a thread pool instead of creating new processes.
// But more fundamentally, threads won't scale as much as you'd like.  If
// your program needs to handle thousands or tens of thousands of
// connections at a time, dealing with tens of thousands of threads will
// not be as efficient as trying to have only a few threads per CPU.
これで一度に複数の接続を扱うことについての完璧な解決法になったのでしょうか? 今すぐこの本を書くのを終わらせて他の作業に取り掛かってよいのでしょうか? そこまでではないです。第一に、いくつかのプラットフォームではプロセス生成(やスレッド生成)についてかなりのコストが掛かったりします。現実には、新しいプロセスを生成する代わりにスレッドプールを使用したいと思うでしょう。でもより根本的なところとして、スレッドは望んでいるほどはスケールしません。もしプログラムが幾千、幾万ものコネクションを同時に扱う必要があるのなら、プログラムは幾千、幾万ものスレッドを処理することになります。そして、それはCPUごとにたった2、3スレッド持たせようとするのと同程度には効率がよいものにはなりません。

// But if threading isn't the answer to having multiple connections, what is?
// In the Unix paradigm, you make your sockets _nonblocking_.  The Unix
// call to do this is:
でも、もしスレッドが複数の接続を持たせることについての解決策でないとするなら何が解決策になるのでしょう? Unixの枠組みでは、ソケットを _非ブロッキング_ にします。これをするためのUnixシステムコールは、
[code,C]
------
fcntl(fd, F_SETFL, O_NONBLOCK);
------
// where fd is the file descriptor for the socket.  footnote:[A file descriptor is
// the number the kernel assigns to the socket when you open it. You use
// this number to make Unix calls referring to the socket.]  Once you've
// made fd (the socket) nonblocking, from then on, whenever you make a
// network call to fd the call will either complete the operation
// immediately or return with a special error code to indicate "I
// couldn't make any progress now, try again."  So our two-socket example
// might be naively written as:
// CHECK: referringはreferの誤り?
です。fdはソケットのファイルディスクリプタです footnote:[ファイルディスクリプタはソケットを開いたときにカーネルがそのソケットに割り当てる番号のことです。この番号はUnixシステムコールがソケットに問い合わせるのに使用します。]。一旦fd(ソケット)を非ブロッキングにすると、以降はfdに対するネットワーク呼び出しをするときはいつでも、その呼び出しは直ちに動作が完了するか、特殊なエラーコード(「今のところ進んでいません。再度試してください」ということを意味する)とともに呼び出しが返るかのどちらかになります。それで2つのソケットを利用する例は単純に書くと次のようになります。

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
// .Bad Example: busy-polling all sockets
.ダメな例: すべてのソケットへの慌ただしい受信問い合わせ
[code,C]
------
/* This will work, but the performance will be unforgivably bad. */
int i, n;
char buf[1024];
for (i=0; i < n_sockets; ++i)
    fcntl(fd[i], F_SETFL, O_NONBLOCK);

while (i_still_want_to_read()) {
    for (i=0; i < n_sockets; ++i) {
        n = recv(fd[i], buf, sizeof(buf), 0);
        if (n == 0) {
            handle_close(fd[i]);
        } else if (n < 0) {
            if (errno == EAGAIN)
                 ; /* The kernel didn't have any data for us to read. */
            else
                 handle_error(fd[i], errno);
         } else {
            handle_input(fd[i], buf, n);
         }
    }
}
------

// Now that we're using nonblocking sockets, the code above would
// _work_... but only barely.  The performance will be awful, for two
// reasons.  First, when there is no data to read on either connection
// the loop will spin indefinitely, using up all your CPU cycles.
// Second, if you try to handle more than one or two connections with
// this approach you'll do a kernel call for each one, whether it has
// any data for you or not.  So what we need is a way to tell the kernel
// "wait until one of these sockets is ready to give me some data, and
// tell me which ones are ready."
では非ブロッキングのソケットを使ってみます。上記のコードは_動く_...のですが、かろうじてに過ぎません。2つの理由でパフォーマンスはひどいものになります。第一に、どちらの接続にも読み込むデータがないときは、CPUサイクルを使い切りながら無限にループを回り続けます。第二に、このやり方で2つ以上の接続を扱おうとすると、何かデータがあるかどうかについてのカーネル呼び出しがそれぞれの接続に対して生じます。したがって、必要なのは「これらのソケットのうちの1つでデータが読み取り可能になるまで待って。それで、どのソケット群が準備ができたのか教えて」とカーネルに伝える方法です。

// The oldest solution that people still use for this problem is
// select().  The select() call takes three sets of fds (implemented as
// bit arrays): one for reading, one for writing, and one for
// "exceptions".  It waits until a socket from one of the sets is ready
// and alters the sets to contain only the sockets ready for use.
// CHECK: fromではなくてinでは?
この問題について今もなお使われている最も古くからある解決法はselect()です。select()呼び出しはfd群に関する3つの集合(ビット配列で実装されている)を引数に取ります。1つは読み込みについて、もう1つは書き込みについて、そして最後が「例外」についての集合です。この呼び出しは3つの集合のどれかの中にあるソケットが準備可能になるまで待ちます。そして、利用可能なソケット群のみ保持するように、これらの集合を改めます。

// Here is our example again, using select:
さらに例を示します。selectを使用します。

//BUILD: FUNCTIONBODY INC:../example_stubs/sec01.h
// .Example: Using select
.例: selectの使用
[code,C]
------
/* If you only have a couple dozen fds, this version won't be awful */
fd_set readset;
int i, n;
char buf[1024];

while (i_still_want_to_read()) {
    int maxfd = -1;
    FD_ZERO(&readset);

    /* Add all of the interesting fds to readset */
    for (i=0; i < n_sockets; ++i) {
         if (fd[i]>maxfd) maxfd = fd[i];
         FD_SET(fd[i], &readset);
    }

    /* Wait until one or more fds are ready to read */
    select(maxfd+1, &readset, NULL, NULL, NULL);

    /* Process all of the fds that are still set in readset */
    for (i=0; i < n_sockets; ++i) {
        if (FD_ISSET(fd[i], &readset)) {
            n = recv(fd[i], buf, sizeof(buf), 0);
            if (n == 0) {
                handle_close(fd[i]);
            } else if (n < 0) {
                if (errno == EAGAIN)
                     ; /* The kernel didn't have any data for us to read. */
                else
                     handle_error(fd[i], errno);
             } else {
                handle_input(fd[i], buf, n);
             }
        }
    }
}
------


// And here's a reimplementation of our ROT13 server, using select() this
// time.
ROT13サーバの再実装を示します。今度はselect()を使用します。

//BUILD: SKIP
// .Example: select()-based ROT13 server
.例: select()ベースのROT13サーバ
[code,C]
------
include::examples_01/01_rot13_server_select.c[]
------

// But we're still not done.  Because generating and reading the select()
// bit arrays takes time proportional to the largest fd that you provided
// for select(), the select() call scales terribly when the number of
// sockets is high.  footnote:[On the userspace side, generating and
// reading the bit arrays takes time proportional to the number of fds
// that you provided for select().  But on the kernel side, reading the
// bit arrays takes time proportional to the largest fd in the bit array,
// which tends to be around _the total number of fds in use in the whole
// program_, regardless of how many fds are added to the sets in
// select().]
でもまだ完成ではありません。なぜならselect()のビット配列の生成および読み込みはselect()に与えた最大のfdに比例して時間が掛かるため、ソケットの数が多くなるとselect()呼び出しはひどく時間が伸びてしまいますfootnote:[ユーザ空間側では、ビット配列の生成および読み込みはselect()に与えたfdの個数に比例します。しかし、カーネル空間側では、ビット配列の読み込みはビット配列中の最大のfdに比例します。その値は、select()の集合にいくつfdを加えたのかに関わらず、大体 _プログラム全体で使用したfdの総数_ になる傾向があります。]。

// Different operating systems have provided different replacement
// functions for select.  These include poll(), epoll(), kqueue(),
// evports, and /dev/poll.  All of these give better performance than
// select(), and all but poll() give O(1) performance for adding a socket,
// removing a socket, and for noticing
// that a socket is ready for IO.
異なるOSで、異なるselectの代替の関数が提供されています。これらの関数にはpoll()、epoll()、kqueue()、evports、/dev/pollを含んでいます。これらすべてはselect()よりもパフォーマンスがよいです。また、poll()を除けば、ソケットの追加や削除、入出力の準備ができたソケットの通知をO(1)のパフォーマンスで行えます。

// Unfortunately, none of the efficient interfaces is a ubiquitous
// standard.  Linux has epoll(), the BSDs (including Darwin) have
// kqueue(), Solaris has evports and /dev/poll... _and none of these
// operating systems has any of the others_.  So if you want to write a
// portable high-performance asynchronous application, you'll need an
// abstraction that wraps all of these interfaces, and provides whichever
// one of them is the most efficient.
残念ながら先の効率的なインターフェースはどれも、どのOSにもある標準ではありません。Linuxにはepoll()が、BSD(Darwinを含む)にはkqueueが、Solarisはevportsと/dev/pollがあります... _それなのに、これらのOSは他が持ってるものをどれも持っていません_ 。ですので、移植性のある高パフォーマンスの非同期アプリケーションを書きたい場合は、これらのインターフェースをラップし、かつ、どのインターフェースでも最も効率的なものとなる抽象化が必要になります。

// And that's what the lowest level of the Libevent API does for you.  It
// provides a consistent interface to various select() replacements,
// using the most efficient version available on the computer where it's
// running.
そして、まさにそれをLibevent API内の最も低いレベルが行います。この部分では様々なselect()の代替群への一貫したインターフェースを提供し、稼働しているコンピュータ上で最も効率的な代替のバージョンを使用します。

// Here's yet another version of our asynchronous ROT13 server.  This
// time, it uses Libevent 2 instead of select().  Note that the fd_sets
// are gone now: instead, we associate and disassociate events with a
// struct event_base, which might be implemented in terms of select(),
// poll(), epoll(), kqueue(), etc.
ここで非同期ROT13サーバのまた別のバージョンを示します。今回はselect()の代わりにLibevent 2を使用します。もはやfd_setがなくなっていることに注意してください。代わりにevent_base構造体を用いてイベントの関連付け、関連付け解除を行います。これはselect()、poll()、epoll()、kqueue()などに基づいて実装されたものかもしれません。

//BUILD: SKIP
// .Example: A low-level ROT13 server with Libevent
.例: Libeventを用いた低レベルのROT13サーバ
[code,C]
-------
include::examples_01/01_rot13_server_libevent.c[]
-------

// (Other things to note in the code: instead of typing the sockets as
// "int", we're using the type evutil_socket_t.  Instead of calling
// fcntl(O_NONBLOCK) to make the sockets nonblocking, we're calling
// evutil_make_socket_nonblocking.  These changes make our code compatible
// with the divergent parts of the Win32 networking API.)
(このコードで注意する他の点として以下のものがあります。まず、ソケットを"int"として型付けする代わりにevutil_socket_t型を使用しています。そして、fcntl(O_NONBLOCK)を呼び出してソケットを非ブロッキングにする代わりに、evutil_make_socket_nonblockingを呼び出しています。これらの変更によってWin32ネットワークAPIにおいて相違する部分についての互換性をコードに持たせます。)


// What about convenience?  (and what about Windows?)
どう便利なのか? (そしてWindowsはどうなのか?)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// You've probably noticed that as our code has gotten more efficient,
// it has also gotten more complex.  Back when we were forking, we didn't
// have to manage a buffer for each connection: we just had a separate
// stack-allocated buffer for each process.  We didn't need to explicitly
// track whether each socket was reading or writing: that was implicit in
// our location in the code.  And we didn't need a structure to track how
// much of each operation had completed: we just used loops and stack
// variables.
ひょっとしたらコードがより効率的になるにつれて、より複雑になっていることに気がついたかもしれません。Forkのところに振り返ると、それぞれの接続に対してバッファを管理する必要はありませんでした。代わりに、それぞれのプロセスに対して、スタック確保のバッファを別々に持っていました。また、それぞれのソケットが読み込み状態なのか書き込み状態なのかを明示的に追跡する必要はありませんでした。それはコードのどの場所かで暗に示されていました。そしてどれくらいそれぞれの動作が完了したかを追跡する構造体は必要ありませんでした。代わりにループとスタック変数を使用していました。

// Moreover, if you're deeply experienced with networking on Windows,
// you'll realize that Libevent probably isn't getting optimal
// performance when it's used as in the example above.  On Windows, the
// way you do fast asynchronous IO is not with a select()-like interface:
// it's by using the IOCP (IO Completion Ports) API.  Unlike all the
// fast networking APIs, IOCP does not alert your program when a socket
// is _ready_ for an operation that your program then has to perform.
// Instead, the program tells the Windows networking stack to _start_ a
// network operation, and IOCP tells the program when the operation has
// finished.
// CHECK: you'reはyou'veの誤り?
さらにWindows上でのネットワークについての経験が深いなら、上の例のようにLibeventを使った場合はおそらく最適なパフォーマンスになっていないことに気づいているでしょう。Windowsでは、高速な非同期入出力を行う方法はselect()のようなインターフェースを用いません。それはIOCP (IO Completion Ports; IO完了ポート) APIを用います。他の高速なネットワークAPIとは異なり、ソケットがプログラムが次に行うべき動作が _準備可能_ であるときにIOCPはプログラムにそのことを通知しません。代わりにプログラムはWindowsネットワーク・スタックにネットワーク動作を _開始する_ ことを伝え、そして、動作が完了したときにIOCPはプログラムにそのことを伝えます。

// Fortunately, the Libevent 2 "bufferevents" interface solves both of
// these issues: it makes programs much simpler to write, and provides
// an interface that Libevent can implement efficiently on Windows _and_
// on Unix.
幸いなことに、Libevent 2の"bufferevent"インターフェースはこれら両方の問題を解決しています。buffereventは、ずっと簡単にプログラムを書くことができるようにし、そして、LibeventがWindows _と_ Unix 上で効率的に実装できるインターフェースを提供します。

// Here's our ROT13 server one last time, using the bufferevents API.
最後にもう一度ROT13サーバを示します。bufferevent APIを使用します。

//BUILD: SKIP
// .Example: A simpler ROT13 server with Libevent
.例: Libeventを用いたより簡単なROT13サーバ
[code,C]
-------
include::examples_01/01_rot13_server_bufferevent.c[]
-------

// How efficient is all of this, really?
ここのすべてのコードの効率性はどうなの? 本当のところ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

// XXXX write an efficiency section here.  The benchmarks on the libevent
// page are really out of date.
XXXXが効率性についてのセクションをここに記載します。Libeventページのベンチマークは実際のところ古くなっています。
